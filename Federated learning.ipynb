{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load and preprocess data from Excel\n",
        "excel_path = '/content/beam_prediction_dataset.xlsx'  # Change this to your file path\n",
        "data = pd.read_excel(excel_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIlKTSqM3hZb",
        "outputId": "0fefbba2-2985-492a-f518-ad6016dab56b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:jax._src.xla_bridge:Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\", line 438, in discover_pjrt_plugins\n",
            "    plugin_module.initialize()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 85, in initialize\n",
            "    options = xla_client.generate_pjrt_gpu_plugin_options()\n",
            "AttributeError: module 'jaxlib.xla_client' has no attribute 'generate_pjrt_gpu_plugin_options'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "# Step 1: Define the Keras model\n",
        "def create_keras_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(5,)),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Step 2: Define the model function\n",
        "def model_fn():\n",
        "    keras_model = create_keras_model()\n",
        "\n",
        "    # TFF model must have forward pass\n",
        "    def forward_pass(model, batch):\n",
        "        x = batch[0]\n",
        "        y = batch[1]\n",
        "        predictions = model(x)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))\n",
        "        return tff.learning.BatchOutput(loss=loss, predictions=predictions)\n",
        "\n",
        "    return forward_pass(keras_model, (tf.random.uniform([32, 5]), tf.random.uniform([32, 1]))) # Dummy data for example\n",
        "\n",
        "# Step 3: Create Federated Averaging process manually\n",
        "def federated_averaging_fn(model, client_optimizer, server_optimizer):\n",
        "    model_weights = model.trainable_variables\n",
        "    client_state = tf.nest.map_structure(lambda x: tf.zeros_like(x), model_weights)\n",
        "\n",
        "    def client_update(model, dataset):\n",
        "        # Simulate the dataset as a tf.data.Dataset\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(32)\n",
        "\n",
        "        # Perform training using the dataset (features, labels)\n",
        "        for batch in dataset:\n",
        "            x, y = batch\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(x)\n",
        "                loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            client_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return model.trainable_variables\n",
        "\n",
        "    def server_update(model_weights, aggregated_weights):\n",
        "        new_weights = [\n",
        "            (1.0 / len(aggregated_weights)) * sum([w for w in aggregated_weights])\n",
        "            for aggregated_weights in zip(*aggregated_weights)\n",
        "        ]\n",
        "        model.set_weights(new_weights)\n",
        "        return model.trainable_variables\n",
        "\n",
        "    def federated_process(state, federated_data):\n",
        "        client_weights = [client_update(model, client_data) for client_data in federated_data]\n",
        "        updated_weights = server_update(model_weights, client_weights)\n",
        "        return updated_weights, model_weights\n",
        "\n",
        "    return federated_process\n",
        "\n",
        "# Step 4: Simulate federated data\n",
        "federated_train_data = [(tf.random.uniform([100, 5]), tf.random.uniform([100, 1]))] * 10  # Simulating 10 clients\n",
        "\n",
        "# Optimizers\n",
        "client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "server_optimizer = tf.keras.optimizers.SGD(learning_rate=1.0)\n",
        "\n",
        "# Define the model (using Keras directly in the model function)\n",
        "model = create_keras_model()\n",
        "\n",
        "# Define federated learning process\n",
        "federated_process = federated_averaging_fn(model, client_optimizer, server_optimizer)\n",
        "\n",
        "# Step 5: Execute federated learning for 10 rounds\n",
        "for round_num in range(10):\n",
        "    # Ensure the federated data is passed as a list of (features, labels) pairs for each client\n",
        "    federated_data = [(tf.random.uniform([100, 5]), tf.random.uniform([100, 1])) for _ in range(10)]\n",
        "    state = federated_process(model.trainable_variables, federated_data)\n",
        "    print(f\"Round {round_num} completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-2qVxp4GglE",
        "outputId": "b9177e25-3d2b-4072-a8fe-dbf4512c0281"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 0 completed.\n",
            "Round 1 completed.\n",
            "Round 2 completed.\n",
            "Round 3 completed.\n",
            "Round 4 completed.\n",
            "Round 5 completed.\n",
            "Round 6 completed.\n",
            "Round 7 completed.\n",
            "Round 8 completed.\n",
            "Round 9 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, federated_data):\n",
        "    total_loss = 0\n",
        "    num_clients = len(federated_data)\n",
        "\n",
        "    # Evaluate the model on each client's data\n",
        "    for client_data in federated_data:\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(client_data).batch(32)\n",
        "        for batch in dataset:\n",
        "            x, y = batch\n",
        "            predictions = model(x)\n",
        "            loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))\n",
        "            total_loss += loss\n",
        "\n",
        "    # Compute average loss over all clients\n",
        "    average_loss = total_loss / num_clients\n",
        "    return average_loss\n",
        "\n",
        "# After training each round, evaluate the model:\n",
        "for round_num in range(10):\n",
        "    state = federated_process(model.trainable_variables, federated_data)\n",
        "    print(f\"Round {round_num} completed.\")\n",
        "\n",
        "    # Evaluate after each round\n",
        "    avg_loss = evaluate_model(model, federated_data)\n",
        "    print(f\"Average loss after round {round_num}: {avg_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BkLBOG6Ggok",
        "outputId": "b6b73140-d84b-45f1-fb29-1391d1c9e019"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 0 completed.\n",
            "Average loss after round 0: 0.3264043927192688\n",
            "Round 1 completed.\n",
            "Average loss after round 1: 0.32531046867370605\n",
            "Round 2 completed.\n",
            "Average loss after round 2: 0.32438474893569946\n",
            "Round 3 completed.\n",
            "Average loss after round 3: 0.3235413432121277\n",
            "Round 4 completed.\n",
            "Average loss after round 4: 0.322805255651474\n",
            "Round 5 completed.\n",
            "Average loss after round 5: 0.32214072346687317\n",
            "Round 6 completed.\n",
            "Average loss after round 6: 0.32157057523727417\n",
            "Round 7 completed.\n",
            "Average loss after round 7: 0.3210555911064148\n",
            "Round 8 completed.\n",
            "Average loss after round 8: 0.3205874264240265\n",
            "Round 9 completed.\n",
            "Average loss after round 9: 0.32013753056526184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HYPERPARAMETER TUNING\n"
      ],
      "metadata": {
        "id": "IFR4N79qP-rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "import tensorflow as tf\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'batch_size': [32, 64, 128]\n",
        "}\n",
        "\n",
        "# Example function for training the model with federated learning\n",
        "def train_with_hyperparameters(learning_rate, batch_size):\n",
        "    model = create_keras_model()\n",
        "    client_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    server_optimizer = tf.keras.optimizers.SGD(learning_rate=1.0)\n",
        "\n",
        "    federated_process = federated_averaging_fn(model, client_optimizer, server_optimizer)\n",
        "\n",
        "    federated_train_data = [(tf.random.uniform([batch_size, 5]), tf.random.uniform([batch_size, 1]))] * 10  # Simulating 10 clients\n",
        "\n",
        "    # Execute federated learning for a few rounds\n",
        "    for round_num in range(10):\n",
        "        federated_data = [(tf.random.uniform([batch_size, 5]), tf.random.uniform([batch_size, 1])) for _ in range(10)]\n",
        "        federated_process(model.trainable_variables, federated_data)\n",
        "\n",
        "    return model  # Return the model after training\n",
        "\n",
        "# Grid search for hyperparameter tuning\n",
        "best_model = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f\"Testing with params: {params}\")\n",
        "    model = train_with_hyperparameters(params['learning_rate'], params['batch_size'])\n",
        "    # Evaluate the model (you can define a proper evaluation function based on your problem)\n",
        "    loss = evaluate_model(model, federated_train_data)\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_model = model\n",
        "\n",
        "print(f\"Best model achieved with loss: {best_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DfTum8nHmpT",
        "outputId": "95f1c9ab-e191-4d93-c90b-6760e12086fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with params: {'batch_size': 32, 'learning_rate': 0.001}\n",
            "Testing with params: {'batch_size': 32, 'learning_rate': 0.01}\n",
            "Testing with params: {'batch_size': 32, 'learning_rate': 0.1}\n",
            "Testing with params: {'batch_size': 64, 'learning_rate': 0.001}\n",
            "Testing with params: {'batch_size': 64, 'learning_rate': 0.01}\n",
            "Testing with params: {'batch_size': 64, 'learning_rate': 0.1}\n",
            "Testing with params: {'batch_size': 128, 'learning_rate': 0.001}\n",
            "Testing with params: {'batch_size': 128, 'learning_rate': 0.01}\n",
            "Testing with params: {'batch_size': 128, 'learning_rate': 0.1}\n",
            "Best model achieved with loss: 0.3558220863342285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL AGGREGATION\n"
      ],
      "metadata": {
        "id": "9GV59v-XP8I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model aggregation function\n",
        "def federated_averaging_fn(model, client_optimizer, server_optimizer):\n",
        "    model_weights = model.trainable_variables\n",
        "    client_state = tf.nest.map_structure(lambda x: tf.zeros_like(x), model_weights)\n",
        "\n",
        "    def client_update(model, dataset):\n",
        "        # Simulate the dataset as a tf.data.Dataset\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(32)\n",
        "\n",
        "        for batch in dataset:\n",
        "            x, y = batch\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(x)\n",
        "                loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            client_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return model.trainable_variables\n",
        "\n",
        "    def server_update(model_weights, aggregated_weights):\n",
        "        new_weights = [\n",
        "            (1.0 / len(aggregated_weights)) * sum([w for w in aggregated_weights])\n",
        "            for aggregated_weights in zip(*aggregated_weights)\n",
        "        ]\n",
        "        model.set_weights(new_weights)\n",
        "        return model.trainable_variables\n",
        "\n",
        "    def federated_process(state, federated_data):\n",
        "        client_weights = [client_update(model, client_data) for client_data in federated_data]\n",
        "        updated_weights = server_update(model_weights, client_weights)\n",
        "        return updated_weights, model_weights\n",
        "\n",
        "    return federated_process\n"
      ],
      "metadata": {
        "id": "QExp5KjXKvii"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Testing and Deployment"
      ],
      "metadata": {
        "id": "8i7_iIlpPuY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def evaluate_model(model, federated_data):\n",
        "    # Assuming federated_data contains the test data for evaluation\n",
        "    total_loss = 0\n",
        "    for data in federated_data:\n",
        "        x, y = data\n",
        "        predictions = model(x)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))\n",
        "        total_loss += loss\n",
        "\n",
        "    avg_loss = total_loss / len(federated_data)\n",
        "    return avg_loss\n",
        "\n",
        "# Deployment example\n",
        "def deploy_model(model):\n",
        "    # Here we save the model and assume a simple deployment process\n",
        "    model.save(\"federated_model.h5\")\n",
        "    print(\"Model saved for deployment\")\n",
        "\n",
        "# After training, test the model\n",
        "test_data = [(tf.random.uniform([100, 5]), tf.random.uniform([100, 1]))] * 10  # Simulated test data\n",
        "avg_loss = evaluate_model(best_model, test_data)\n",
        "print(f\"Final evaluation loss: {avg_loss}\")\n",
        "\n",
        "# Deploy the model\n",
        "deploy_model(best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R33s0Xw1Kvk9",
        "outputId": "eb14d67d-3d16-4d03-93cd-280784d710e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final evaluation loss: 0.08864980936050415\n",
            "Model saved for deployment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling Federated Learning"
      ],
      "metadata": {
        "id": "5xz82X2zPqBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Updated client update function\n",
        "def client_update(model, dataset, client_optimizer):\n",
        "    # Simulate the dataset as a tf.data.Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(32)\n",
        "\n",
        "    # Ensure the optimizer is built with model variables\n",
        "    client_optimizer.build(model.trainable_variables)\n",
        "\n",
        "    for batch in dataset:\n",
        "        x, y = batch\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x)\n",
        "            loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        client_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return model.trainable_variables\n",
        "\n",
        "\n",
        "# Model Aggregation (Federated Averaging)\n",
        "def federated_averaging_fn(model, client_optimizer, server_optimizer):\n",
        "    model_weights = model.trainable_variables\n",
        "    client_state = tf.nest.map_structure(lambda x: tf.zeros_like(x), model_weights)\n",
        "\n",
        "    def client_update_fn(model, federated_data):\n",
        "        # Here we run client_update for each dataset from different clients\n",
        "        return [client_update(model, data, client_optimizer) for data in federated_data]\n",
        "\n",
        "    def server_update_fn(model_weights, client_weights):\n",
        "        # Averaging model weights\n",
        "        averaged_weights = [\n",
        "            tf.reduce_mean([client_weight[i] for client_weight in client_weights], axis=0)\n",
        "            for i in range(len(client_weights[0]))\n",
        "        ]\n",
        "        model.set_weights(averaged_weights)\n",
        "        return model.trainable_variables\n",
        "\n",
        "    def federated_process(state, federated_data):\n",
        "        client_weights = client_update_fn(model, federated_data)\n",
        "        updated_weights = server_update_fn(model_weights, client_weights)\n",
        "        return updated_weights, model_weights\n",
        "\n",
        "    return federated_process\n"
      ],
      "metadata": {
        "id": "bBj1VEX-Kvm7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate large-scale federated learning\n",
        "def simulate_large_scale_federated_learning(num_clients=100):\n",
        "    federated_train_data = [(tf.random.uniform([32, 5]), tf.random.uniform([32, 1]))] * num_clients  # Simulated 100 clients\n",
        "\n",
        "    # Using the previously defined federated_averaging_fn\n",
        "    client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "    server_optimizer = tf.keras.optimizers.SGD(learning_rate=1.0)\n",
        "    federated_process = federated_averaging_fn(best_model, client_optimizer, server_optimizer)\n",
        "\n",
        "    # Execute federated learning for more clients (scaling up)\n",
        "    for round_num in range(10):\n",
        "        federated_data = [(tf.random.uniform([32, 5]), tf.random.uniform([32, 1])) for _ in range(num_clients)]\n",
        "        federated_process(best_model.trainable_variables, federated_data)\n",
        "\n",
        "    print(f\"Federated Learning completed for {num_clients} clients\")\n",
        "\n",
        "# Simulate large-scale federated learning with 100 clients\n",
        "simulate_large_scale_federated_learning(num_clients=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBjzGLOqKvqS",
        "outputId": "c4530ec0-74a9-45b3-9858-97abed54dab6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federated Learning completed for 100 clients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zxr0bp4qJQa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9VH1D_vFbJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWh5ZyDDCPqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzfQ53d-72cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgTfn1Qk72e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFHl1dLU72hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OgdUaO7m72j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHDvzUTR72nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgFZA7__zX4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_AuND-8czX8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}